üíª Repository Agent Instructions: Express/Next.js/PostgreSQL
This repository is built on a Node.js/Express backend (for API/business logic), a Next.js frontend (for SSR/SSG and the 4-page site generation), and PostgreSQL for data persistence. Its primary responsibility is to ingest user resume/information input and generate structured, detailed content, along with rendering the four required website pages: Home, About, Portfolio, and Contact Me.
üõ†Ô∏è Code Standards
Required Before Each Commit
‚Ä¢ Run npm run format (or yarn format) before committing any changes to ensure consistent code styling across JavaScript, TypeScript, and CSS/SCSS files. This typically uses Prettier.
‚Ä¢ Run npm run lint (or yarn lint) to verify code quality against configured rules (e.g., ESLint).
Development Flow
‚Ä¢ Install Dependencies: npm install (or yarn install)
‚Ä¢ Run Backend (API): npm run dev:api (Starts the Express server)
‚Ä¢ Run Frontend (Next.js): npm run dev:web (Starts the Next.js development server)
‚Ä¢ Run Tests: npm run test (Executes unit and integration tests)
‚Ä¢ Full CI Check: npm run ci (Includes linting, formatting check, and testing)
üìÇ Repository Structure
The architecture separates the server-side API, the Next.js application, and the database setup.
‚Ä¢ **api/**: Contains the Express/Node.js backend logic.
‚Ä¢ api/src/: Core backend source code.
‚Ä¢ api/src/controllers/: API route handlers and business logic entry points.
‚Ä¢ api/src/services/: Services for data processing, content generation, and third-party integrations.
‚Ä¢ api/src/models/: Sequelize/Prisma models or raw SQL interfaces for PostgreSQL interaction.
‚Ä¢ **web/**: Contains the Next.js frontend application.
‚Ä¢ web/pages/: The core page files (index.tsx, about.tsx, portfolio.tsx, contact.tsx).
‚Ä¢ web/components/: Reusable React components.
‚Ä¢ web/styles/: Global styles and SCSS/CSS modules.
‚Ä¢ web/utils/: Frontend-specific utility functions.
‚Ä¢ **database/**: Database configuration, migrations, and seed files.
‚Ä¢ database/migrations/: Knex/TypeORM/Prisma migration scripts.
‚Ä¢ database/seeds/: Initial data population scripts.
‚Ä¢ config/: Environment configuration files.
‚Ä¢ docs/: Project documentation (e.g., API specifications, deployment guides).
‚Ä¢ testing/: Mocks, fixtures, and specific end-to-end (E2E) test helpers.
‚Ä¢ **types/**: Shared TypeScript definitions and interfaces used by both api/ and web/.
üîë Key Guidelines for Contribution
1. TypeScript First: All new functionality should be written in TypeScript for strong typing and better maintainability, especially for the OpenAPI document parsing/transformation utilities you are developing.
2. API/Web Separation: Ensure a clear separation of concerns. Express handles data ingestion, processing, and content generation logic. Next.js focuses on data fetching and rendering the generated content into the static page structure.
3. Data Integrity: All PostgreSQL interactions must leverage ORM/Query Builder capabilities (like Sequelize or Prisma) and be wrapped in secure, transactional logic.
4. Content Generation Logic: The content generation service (api/src/services/) must be robust and auditable, transforming the raw resume input into a structured content model before persistence.
5. Testing: Implement unit tests for all backend services and utility functions. For critical content generation logic, use parameterized tests (similar to table-driven tests) to cover diverse input scenarios.
6. Documentation: Document public APIs (e.g., using JSDoc or Swagger/OpenAPI specifications, linking to your OpenAPI tooling). Ensure any complex content generation algorithms are clearly explained in the docs/ folder.

========================Expanded Copilot Instructions=======================
========= Ways to get Cost free/Effective AI Data Model Api's Access========

That is a key challenge for any developer building a content-rich application like yours, Rao, especially when seeking a zero-cost operational model. Given your focus on Python/Replit and the Express/Next.js/PostgreSQL stack, the goal is to leverage either truly free tiers or open-source solutions where you cover the hosting cost, but not the API fees.
There are primarily two analytical paths to achieving a free-of-cost AI model API for content generation: Leveraging Free Tiers/Grants or Self-Hosting Open-Source LLMs (The "Free-as-in-Compute" approach).
1. üÜì Leveraging Free Tiers and Developer Grants
Many major LLM providers offer a free tier sufficient for initial development, prototyping, and low-volume usage. This is the simplest path to getting high-quality output quickly, using well-documented Node.js/TypeScript SDKs.
| Provider | Model Example | Free Offering Details | Constraint/Trade-off |
|---|---|---|---|
| OpenAI/Others | GPT-3.5 Turbo, Claude, Gemini | Often offers a free credit balance upon signup for new developers (e.g., $5 to $20 USD). | Credits expire or are quickly consumed. This is not a sustainable long-term free solution. |
| Hugging Face | Various models (Mistral, Llama) | The Inference API for some models has a generous free tier or is free for certain smaller models. | Rate limits and response latency can be high on the free tier. |
| Google Gemini API | Gemini 2.5 Flash, etc. | Offers a Free Tier for specific models up to a certain request per minute (RPM) and requests per day (RPD) limit. | Requires a Google Cloud Project setup, but the API calls themselves can remain free up to the stated limits. |
| Specialized Resume APIs | UseResume AI, etc. | Some platforms offer a small amount of free API credits (e.g., 30 free credits) for trial purposes. | Extremely limited volume and designed for conversion to a paid model. |
Recommendation: Start with the Google Gemini API Free Tier for rapid prototyping, as it offers a modern, high-quality model with specific usage limits before incurring costs, making it the most sustainable "API" approach.
2. üß† Self-Hosting Open-Source LLMs (The True Free Approach)
This path gives you complete control and is genuinely free in terms of licensing, but it shifts the cost to compute/hosting. This aligns with your interest in a Python suite and development on platforms like Replit, as you can deploy your own model interface.
A. Model Selection (Open-Source LLMs)
You must select a small, efficient LLM licensed for commercial use (like Apache 2.0). These are designed to run on consumer-grade hardware or small cloud instances.
 * Mistral-7B Instruct: Excellent performance for its size.
 * Llama 3 (8B): A highly capable and popular model for general-purpose tasks.
 * Gemma (2B or 7B): Google‚Äôs lightweight family of models.
B. Deployment Frameworks
To serve one of these models and expose a simple API endpoint for your Express backend to consume, you can use specialized frameworks:
| Framework | Description | Integration Method | Relevance to Your Stack |
|---|---|---|---|
| Ollama | Simplifies downloading, running, and managing open-source LLMs locally or on a server. It provides a simple REST API (default port 11434). | Your Express backend makes a standard fetch or Node.js HTTP request to the local Ollama API. | High. Can be containerized with Docker for easy deployment on a VPS/cloud, which is central to your tech interests (Docker/Kubernetes). |
| vLLM / Text Generation Inference (TGI) | Designed for high-performance, large-scale serving of LLMs. Overkill for low volume, but highly optimized. | Similar REST API exposure. | Moderate. Useful if you expect significant traffic and need optimization. |
| Hugging Face Transformers | Direct Python code to load and run the model. You'd wrap this in a simple Python Flask/FastAPI server to expose an API. | Custom FastAPI endpoint consumed by your Express app. | High. Aligns with your Python crawling/testing suite and is a direct, hands-on method. |
C. Free/Low-Cost Hosting for Self-Hosting
The main hurdle is hosting the LLM server. The model binaries (e.g., a 7B model) are often several gigabytes, requiring a decent amount of RAM/VRAM.
 * Replit/Render/Fly.io (Free Tiers):
   * Replit is feasible for small-scale development and testing since it offers generous compute for small projects, aligning with your Python suite goals. However, persistent, high-uptime API serving for an LLM might exceed free tier limits or available resources.
   * Fly.io and Render occasionally have free tiers that could support a very small LLM for short periods, but expect limitations on CPU/RAM that might throttle performance.
 * Cloud Free Tier (AWS/Azure):
   * AWS Free Tier: You might be able to run a tiny quantized model (e.g., a 2-3B parameter model) on a free-tier t2.micro/t3.micro EC2 instance, but this is challenging due to limited RAM and CPU burst capacity. Expect slow inference times. You would expose the Ollama/FastAPI server to the internet via the EC2 instance.
Summary: Recommended Path for Rao
Given your focus on an analytical, robust solution and your use of Docker/Kubernetes in your professional life, I recommend the following structured approach:
 * Initial Prototyping (Fast & Easy): Use the Google Gemini API Free Tier for immediate, high-quality results while you build out the content generation logic in your Express backend.
 * Sustainable Free Solution (Developer Focus): Configure the content generation layer in your Express backend to be model-agnostic.
   * Deploy Ollama on a small VPS or an experimental cloud instance.
   * Serve an open-source model like Mistral-7B via the Ollama API.
   * Point your Node.js service to this self-hosted endpoint. This gives you a truly free content generation source where you only manage the minimal cost of the underlying compute, not transaction fees.
Would you like to explore how to structure the prompt engineering for a resume-focused LLM, using your existing user data fields?

